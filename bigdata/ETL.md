**ETL**，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过萃取（extract）、转置（transform）、加载（load）至目的端的过程。**ETL**一词较常用在[数据仓库](http://baike.baidu.com/view/19711.htm)，但其对象并不限于[数据仓库](http://baike.baidu.com/view/19711.htm)。





ETL是数据仓库中的非常重要的一环。 它是承前启后的必要的一步。相对于关系数据库， 数据仓库技术没有严格的数学理论基础，它更面向实际工程应用。 所以从工程应用的角度来考虑， 按着物理数据模型的要求加载数据并对数据进行一些系列处理， 处理过程与经验直接相关， 同时这部分的工作直接关系数据仓库中数据的质量， 从而影响到联机分析处理和数据挖掘的结果的质量。

数据仓库是一个独立的数据环境， 需要通过抽取过程将数据从联机事务处理环境、 外部数据源和脱机的数据存储介质导入到数据仓库中；在技术上， ETL主要涉及到**关联、转换、增量、调度和监控**等几个方面； 数据仓库系统中数据不要求与联机事务处理系统中数据实时同步， 所以ETL可以定时进行。但多个ETL的操作时间、 顺序和成败对数据仓库中信息的有效性至关重要。

**ETL中的关键技术**

ETL过程中的主要环节就是数据抽取、数据转换和加工、数据装载。为了实现这些功能，各个ETL工具一般会进行一些功能上的扩充，例如工作流、调度引擎、规则引擎、脚本支持、统计信息等。

## 数据抽取

数据抽取是从数据源中抽取数据的过程。实际应用中，数据源较多采用的是关系数据库。从数据库中抽取数据一般有以下几种方式。

### (1)全量抽取

全量抽取类似于数据迁移或数据复制，它将数据源中的表或视图的数据原封不动的从数据库中抽取出来，并转换成自己的ETL工具可以识别的格式。全量抽取比较简单。

### (2)增量抽取

增量抽取只抽取自上次抽取以来数据库中要抽取的表中新增或修改的数据。在ETL使用过程中。增量抽取较全量抽取应用更广。**如何捕获变化的数据**是增量抽取的关键。对捕获方法一般有两点要求：

- **准确性**，能够将业务系统中的变化数据按一定的频率准确地捕获到；

- **性能，**不能对业务系统造成太大的压力，影响现有业务。

  

目前增量数据抽取中常用的捕获变化数据的方法有：

#### **a.触发器**：

**在**要抽取的**表上建立**需要的**触发器**，一般要建立**插入**、**修改**、**删除**三个触发器，每当源表中的数据发生变化，就被相应的**触发器将变化的数据写入一个临时表，抽取线程从临时表中抽取数据，临时表中抽取过的数据被标记或删除**。触发器方式的优点是数据抽取的性能较高，缺点是要求业务表建立触发器，对业务系统有一定的影响。

#### **b.时间戳：**

它是一种**基于快照比较**的变化数据捕获方式，在源表上增加一个时间戳字段，系统中更新修改表数据的时候，同时修改时间戳字段的值。当进行数据抽取时，通过比较系统时间与时间戳字段的值来决定抽取哪些数据。有的数据库的时间戳支持自动更新，即表的其它字段的数据发生改变时，自动更新时间戳字段的值。有的数据库不支持时间戳的自动更新，这就要求业务系统在更新业务数据时，手工更新时间戳字段。同触发器方式一样，时间戳方式的性能也比较好，数据抽取相对清楚简单，但对业务系统也有很大的侵入性（加入额外的时间戳字段），特别是对不支持时间戳的自动更新的数据库，还要求业务系统进行额外的更新时间戳操作。另外，无法捕获对时间戳以前数据的delete和update操作,在数据准确性上受到了一定的限制。

#### **c.全表比对：**

典型的全表比对的方式是采用**MD5校验码**。ETL工具事先为要抽取的表建立一个结构类似的MD5临时表，该临时表记录源表主键以及根据所有字段的数据计算出来的MD5校验码。每次进行数据抽取时，对源表和MD5临时表进行MD5校验码的比对，从而决定源表中的数据是新增、修改还是删除，同时更新MD5校验码。MD5方式的优点是对源系统的倾入性较小（仅需要建立一个MD5临时表），但缺点也是显而易见的，与触发器和时间戳方式中的主动通知不同，MD5方式是被动的进行全表数据的比对，性能较差。当表中没有主键或唯一列且含有重复记录时，MD5方式的准确性较差。

**d.日志对比**：通过分析数据库自身的日志来判断变化的数据。Oracle的改变数据捕获（**CDC，Changed Data Capture**）技术是这方面的代表。CDC 特性是在Oracle9i数据库中引入的。CDC能够帮助你识别从上次抽取之后发生变化的数据。利用CDC，在对源表进行insert、update或 delete等操作的同时就可以提取数据，并且变化的数据被保存在数据库的变化表中。这样就可以捕获发生变化的数据，然后利用数据库视图以一种可控的方式提供给目标系统。CDC体系结构基于**发布者/订阅者**模型。发布者捕捉变化数据并提供给订阅者。订阅者使用从发布者那里获得的变化数据。通常，CDC系统拥有一个发布者和多个订阅者。发布者首先需要识别捕获变化数据所需的源表。然后，它捕捉变化的数据并将其保存在特别创建的变化表中。它还使订阅者能够控制对变化数据的访问。订阅者需要清楚自己感兴趣的是哪些变化数据。一个订阅者可能不会对发布者发布的所有数据都感兴趣。订阅者需要创建一个订阅者视图来访问经发布者授权可以访问的变化数据。CDC分为同步模式和异步模式，同步模式实时的捕获变化数据并存储到变化表中，发布者与订阅都位于同一数据库中。异步模式则是基于Oracle的流复制技术。

ETL处理的数据源除了关系数据库外，还可能是文件，例如txt文件、excel文件、xml文件等。对文件数据的抽取一般是进行全量抽取，一次抽取前可保存文件的时间戳或计算文件的MD5校验码，下次抽取时进行比对，如果相同则可忽略本次抽取。

## 数据转换和加工

从数据源中抽取的数据不一定完全满足目的库的要求，例如数据格式的不一致、数据输入错误、数据不完整等等，因此有必要对抽取出的数据进行数据转换和加工。

数据的转换和加工可以在ETL引擎中进行，也可以在数据抽取过程中利用关系数据库的特性同时进行。

### (1)ETL引擎中的数据转换和加工

ETL引擎中一般以组件化的方式实现数据转换。常用的数据转换组件有字段映射、数据过滤、数据清洗、数据替换、数据计算、数据验证、数据加解密、数据合并、数据拆分等。这些组件如同一条流水线上的一道道工序，它们是可插拔的，且可以任意组装，各组件之间通过数据总线共享数据。

有些ETL工具还提供了脚本支持，使得用户可以以一种编程的方式定制数据的转换和加工行为。

### (2)在数据库中进行数据加工

关系数据库本身已经提供了强大的SQL、函数来支持数据的加工，如在SQL查询语句中添加where条件进行过滤，查询中重命名字段名与目的表进行映射，substr函数，case条件判断等等。下面是一个SQL查询的例子。

select ID as USERID, substr(TITLE, 1, 20) as TITLE, case when REMARK is null then ' ' else REMARK end as CONTENT from TB_REMARK where ID > 100;

相比在ETL引擎中进行数据转换和加工，直接在SQL语句中进行转换和加工更加简单清晰，性能更高。对于SQL语句无法处理的可以交由ETL引擎处理。

## 数据装载

将转换和加工后的数据装载到目的库中通常是ETL过程的最后步骤。装载数据的最佳方法取决于所执行操作的类型以及需要装入多少数据。当目的库是关系数据库时，一般来说有两种装载方式：

(1)直接SQL语句进行insert、update、delete操作。

(2)采用批量装载方法，如bcp、bulk、关系数据库特有的批量装载工具或api。

大多数情况下会使用第一种方法，因为它们进行了日志记录并且是可恢复的。但是，批量装载操作易于使用，并且在装入大量数据时效率较高。使用哪种数据装载方法取决于业务系统的需要。



ETL工具的典型代表有:Informatica、Datastage、ODI ,OWB、 微软DTS、 Beeload、 Kettle、久其ETL……

## ETL工具

旗鼓相当：Datastage与Powercenter：
就Datastage和Powercenter而言，这两者目前占据了国内市场绝大部分的份额，在成本上看水平相当，虽然市面上还有诸如Business Objects公司的Data Integrator、Cognos公司的DecisionStream，但尚属星星之火，未成燎原之势。

在两大ETL工具技术的比对上，可以**从对ETL流程的支持、对元数据的支持、对数据质量的支持、维护的方便性、定制开发功能的支持等方面考虑。**

一个项目中，从数据源到最终目标表，多则上百个ETL过程，少则也有十几个。这些过程之间的依赖关系、出错控制以及恢复的流程处理，都是工具需要重点考虑。在这一方面，Datastage的早期版本对流程就缺乏考虑，而在6版本则加入Job Sequence的特性，可以将Job、shell脚本用流程图的方式表示出来，依赖关系、串行或是并行都可以一目了然，就直观多了。Powercenter有Workflow的概念，也同样可以将Session串联起来，这和Datastage Sequence大同小异。

**ETL的元数据包括数据源、目标数据的结构、转换规则以及过程的依赖关系等**。在这方面，Datastage和Powercenter从功能上看可谓不分伯仲，只是后者的元数据更加开放，存放在关系数据库中，可以很容易被访问（Informatic把Metadata全部放在数据库中而Datastage是自己管理Metadata，不依赖任何数据库.）。此外，这两个厂家又**同时提供专门的元数据管理工具，Ascential有Metastage，而Informatica拥有Superglue**。

数据质量方面，两种产品都采用同样的策略——独立出ETL产品之外，另外有专门的数据质量管理产品。例如和Datastage配套用的有ProfileStage和QualityStage，而Informatica最近也索性收购了原先OEM的数据质量管理产品FirstLogic。而在它们的ETL产品中，只是在Job或是Session前后留下接口，所谓前过程、后过程，虽然不是专为数据质量预留的接口，不过至少可以利用它外挂一些数据质量控制的模块。

在具体实现上看，Datastage通过Job实现一个ETL过程，运行时可以通过指定不同参数运行多个实例。Powercenter通过Mapping表示一个ETL过程，运行时为Session，绑定了具体的物理数据文件或表。在修改维护上，这两个工具都是提供图形化界面。这样的好处是直观、傻瓜式的；不好的地方就是改动还是比较费事（特别是批量化的修改）。

定制开发方面，两者都提供抽取、转换插件的定制，但笔者认为，Datastage的定制开发性要比Powercenter要强那么一点点。因为**Datastage至少还内嵌一种类BASIC语言，可以写一段批处理程序来增加灵活性，而Powercenter似乎还缺乏这类机制**。另外从参数控制上，虽然两者的参数传递都是比较混乱的，但Datastage至少可以对每个job设定参数，并且可以job内部引用这个参数名；而Powercenter显得就有些偷懒，参数放在一个参数文件中，理论上的确可以灵活控制参数，但这个灵活性需要你自己更新文件中的参数值（例如日期更新）。另外，Powercenter还不能在mapping或session中引用参数名，这一点就让人恼火。

总起来看，Datastage和Powercenter可谓旗鼓相当，在国内也都有足够的支持能力，Datastage在2005年被IBM收购之后，可以说后劲十足。而Informatica则朝着BI全解决方案提供商方向发展，Powercenter显然还将是它的核心产品。

## ODI

ODI提出了**知识模块**的概念，把这些场景的详细的实现步骤作为一个一个的知识模块并使用Jython脚本语言结合数据库的SQL语句录制成一步一步的步骤忠实地记录下来，这样就形成了ODI里的100多个知识模块，基本上包含了所有普通应用所涉及到的所有场景。更方便的是，用户既可以直接使用ODI的知识模块完成数据的获取工作，也可以直接在知识模块上面做各种定制，比如某一个业务场景可能并不需要知识模块里的某一个特定的步骤，那就可以直接把该步骤删除掉从而提供更好的性能。当然用户也可以完全自己来开发这些知识模块。

ODI的知识模块主要分为几个大类（RKM,CKM,LKM,IKM,SKM），其中最重要的是LKM（load KM）和IKM(Integration KM)RKM。
RKM:完成从源系统和目标系统的数据结构的反向工程来形成数据模型的功能。
CKM:CKM完成数据质量检查。
LKM：LKM完成从源数据库数据加载到临时表。
IKM：IKM完成从临时表的数据加载到目标表。
SKM：SKM完成ODI和WEB服务接口的功能。

ODI的性能不是很好，Powercenter > Datastage > ODI

## 独树一帜：Teradata的ETL Automation

继续要说的第三种产品是Teradata的ETL Automation。之所以拿它单独来说是因为它和前面两种产品的体系架构都不太一样。与其说它是ETL工具，不如说是提供了一套ETL框架。它没有将注意力放在如何处理“转换”这个环节上，而是利用Teradata数据库本身的并行处理能力，用SQL语句来做数据转换的工作，其重点是提供对ETL流程的支持，包括前后依赖、执行和监控等。

这样的设计和Datastage、Powercenter风格迥异，后两者给人的印象是具有灵活的图形化界面，开发者可以傻瓜式处理ETL工作，它们一般都拥有非常多的“转换”组件，例如聚集汇总、缓慢变化维的转换。而对于Teradata的ETL Automation，有人说它其实应该叫做ELT，即装载是在转换之前的。的确，如果依赖数据库的能力去处理转换，恐怕只能是ELT，因为转换只能在数据库内部进行。从这个角度看，Automation对数据库的依赖不小，似乎是一种不灵活的设计。也正是这个原因，考虑它的成本就不单单是ETL产品的成本了。

其实，在购买现成的工具之外，还有自己从头开发ETL程序的。

ETL工作看起来并不复杂，特别是在数据量小、没有什么转换逻辑的时候，自己开发似乎非常节省成本。的确，主流的ETL工具价格不菲，动辄几十万；而从头开发无非就是费点人力而已，可以控制。至于性能，人大多是相信自己的，认为自己开发出来的东西知根知底，至少这些程序可以完全由自己控制。

就目前自主开发的ETL程序而言，有人用c语言编写，有人用存储过程，还有人用各种语言混杂开发，程序之间各自独立。这很危险，虽然能够让开发者过足编码的瘾，却根本不存在架构。

有位银行的朋友，他们几年前上的数据仓库系统，就是集成商自己用c语言专门为他们的项目开发的。单从性能上看似乎还不赖，然而一两年下来，项目组成员风雨飘零，早已物是人非，只有那套程序还在那里；而且，按照国内目前的软件工程惯例，程序注释和文档是不全或者是不一致的，这样的程序已经对日常业务造成很大阻碍。最近，他们已经开始考虑使用ETL工具重新改造了。

## 国产ETL软件—udis睿智ETL：

​    再来看国产的, 采用SOA架构体系，具有更好的方便性和灵活性.缺点是配置复杂，缺少对元数据的管理。

总体比较列表：

![image-20211015102939326](https://raw.githubusercontent.com/jingbiao95/Images/main/image-20211015102939326.png)

## ETL工具的选择

在数据集成中该如何选择ETL工具呢？一般来说需要考虑以下几个方面：

(1)对平台的支持程度。

(2)对数据源的支持程度。

(3)抽取和装载的性能是不是较高，且对业务系统的性能影响大不大，倾入性高不高。

(4)数据转换和加工的功能强不强。

(5)是否具有管理和调度功能。

(6)是否具有良好的集成性和开放性


# 参考文献

[ETL介绍与ETL工具比较](https://blog.csdn.net/u013412535/article/details/43462537)